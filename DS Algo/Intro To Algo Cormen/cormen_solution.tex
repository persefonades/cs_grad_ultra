% cormen_solution.tex
\documentclass[fontsize=12pt,paper=a4,open=any]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx} 
% packages for math functions
\usepackage{amsmath}

\usepackage{xcolor}
\usepackage{listings}
% packages for algorithm
% \usepackage[noend]{algpseudocode}
\usepackage[linesnumbered,ruled]{algorithm2e}
% define variables for algorithm 
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Break}{break}
\SetKw{To}{to}
\SetKw{DownTo}{downto}
\SetKw{And}{and}
\SetKw{Or}{or}
\SetKwFunction{Func}{Fn}
% algorithm settings
\DontPrintSemicolon

\begin{document}
\title{Introduction to Algorithms - Solutions \\
\large 3rd Edition}
\author{Purbayan Chowdhury}

\maketitle

\tableofcontents

\chapter{The Role of Algorithms in Computing}

\chapter{Getting Started}

\section{Insertion Sort}
\begin{enumerate}
	\item[\textbf{Ex 2.1-1}]

	\item[\textbf{Ex 2.1-2}]
		\textbf{Rewrite the INSERTION-SORT procedure to sort into non-increasing instead of non-decreasing order.} \

	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Unsorted\ Array}
			\Output{A $\longleftarrow$ Array\ Sorted\ in\ Non-increasing Order}

			\For{$j \longleftarrow 1$ \To $A.length-1$}
			{
				$key\longleftarrow A[j]$\;
				\tcc{Insert A[j] into the sorted sequence A[1..j-1]}
				$i \longleftarrow j-1$\;
				\While{$i \ge 0$ \And $A[i] > key$}
				{
					$A[i+ 1] \longleftarrow A[i]$\;
					$i \longleftarrow i-1$\;
				}
				$A[i+ 1] \longleftarrow key$\;
			}

			\caption{Non-increasingInsertionSort}
		\end{algorithm}
	\item[\textbf{Ex 2.1-3}]
	
	\item[\textbf{Ex 2.1-4}]
		\textbf{Consider the searching problem:\
			Input: A sequence of n numbers $A =(a_1, a_2,\dots,a_n)$\
			Output: An index $i$ such that $v=A[i]$ or the special value NIL if $v$ does not appear in $A$.\
			Write pseudocode for linear search, which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfils the three necessary properties.
		}
	
	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Array\\
			v $\longleftarrow$ value to be searched}
			\Output{i $\longleftarrow$ index of the value if found, else NIL}

			$i \longleftarrow NIL$\;
			\For{$j \longleftarrow 0$ \To $A.length-1$}
			{
				\If{$A[j]=v$}
				{
					$i \longleftarrow j$\;
					\Break\;
				}
			}
			\Return $i$

			\caption{Linear-Search}
		\end{algorithm}

	\item[\textbf{Ex. 2.1-5}]
		\textbf{Consider the problem of adding two $n$-bit binary integers, stored in two n-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an ($n+1$)-element array $C$. State the problem formally and write pseudocode for adding the two integers.}

	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ First Array\\
			B $\longleftarrow$ Second Array}
			\Output{C $\longleftarrow$ Binary Addition Result}

			$carry \longleftarrow 0$\;
			\For{$i \longleftarrow n-1$ \DownTo $0$}
			{
				$C[i+1] \longleftarrow (A[i] + B[i]+carry) (\bmod 2)$\;
				\If{$A[i] + B[i] + carry \ge 2$}
				{
					$carry \longleftarrow 1$\;
				}
				\Else
				{
					$carry \longleftarrow 0$\;
				}
			}
			$C[0] \longleftarrow carry$\;
			\caption{$n$-bitBinaryAddition}
		\end{algorithm}

\end{enumerate}

\section{Analyzing algorithms}
\begin{enumerate}
	\item[\textbf{Ex 2.2-1}]

	\item[\textbf{Ex 2.2-2}]
		\textbf{Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n-1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort. What loop invariant does this algorithm maintain? Why does it need to run for only the first $n-1$ elements, rather than for all n elements? Give the best-case and worst-case running times of selection sort in $\Theta$-notation.}

	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Unsorted Array}
			\Output{A $\longleftarrow$ Array Sorted in Increasing Order}

			\DontPrintSemicolon

			\For{$i \longleftarrow 0$ \To $n-1$}
			{
				$min \longleftarrow i$\;
				\For{$j \longleftarrow i+1$ \To $n$}
				{
					\tcc{Find the index of the ith smallest element}
					\If{$A[j] < A[min]$}
					{
						$min \longleftarrow j$\;
					}
				}
				Swap $A[min]$ and $A[i]$\;
			}
			\caption{SelectionSort}
		\end{algorithm}

			The loop invariant of selection sort is as follows:\\
			At each iteration of the for loop of lines 1 through 9, the subarray $A[0 \dots i-1]$ contains the $i-1$ smallest elements of $A$ in increasing order. After $n-1$ iterations of the loop, the $n-1$ smallest elements of $A$ are in the first $n-1$ positions of $A$ in increasing order so the nth element is necessarily the largest amount.\\
			The best-case and worst-case running times of selection sort are $\Theta(n^2)$, this is because regardless of how the elements are initially arranged, on the $i$-th iteration of the for loop in line 1, always inspects each of the remaining $n-i$ elements to find the smallest one remaining. \\
			This yields a running \\
			$\sum_{i=1}^{n-1} n-i = n(n-1) - \sum_{i=1}^{n-1} i = n^2-n-\frac{n^2-n}{2} = \frac{n^2-n}{2} = \Theta(n^2)$

		\item[\textbf{Ex 2.2-3}]

		\item[\textbf{Ex 2.2-4}]
\end{enumerate}

\section{Designing algorithms}
\begin{enumerate}
	\item[\textbf{Ex 2.3.1}]
		\textbf{Illustrate the operation of merge sort on the array A=(3, 41, 52, 26, 38, 57, 9, 49)}

	\item[\textbf{Ex 2.3.2}]
		\textbf{Rewrite the MERGE procedure so that it does not use sentinels, instead stopping once either array L or R has had all its elements copied back to A and then copying the remainder of the other array back into A.}

	\item[A.]
		\begin{algorithm}[H]
			\SetKwInOut{Input}{Input}
			\SetKwInOut{Output}{Output}
			\Input{A $\longleftarrow$ Unsorted Array\\
			p $\longleftarrow$ start index\\
			q $\longleftarrow$ middle index\\
			r $\longleftarrow$ end index}
			\Output{A $\longleftarrow$ Array Sorted in Increasing Order}

			$n1 \longleftarrow q-p+1$\;
			$n2 \longleftarrow r-q$\;
			let $L[1,\dots,n1]$ and $R[1,\dots,n2]$ be new arrays\;
			\For{$i \longleftarrow 0$ \To $n1-1$}
			{
				$L[i] \longleftarrow A[p+i]$\;
			}
			\For{$j \longleftarrow 0$ \To $n2-1$}
			{
				$R[j] \longleftarrow A[q+j+i]$\;
			}

			$i \longleftarrow 0$\;
			$j \longleftarrow 0$\;
			$k \longleftarrow p$\;

			\While{$i \neq n1$ \And $j \neq n2$}
			{
				\If{$L[i] \leq R[j]$}
				{
					$A[k] \longleftarrow L[i]$\;
					$i \longleftarrow i+1$\;
				}
				\Else
				{
					$A[k] \longleftarrow R[j]$\;
					$j \longleftarrow j+1$\;
				}
				$k \longleftarrow k+1$\;
			}
			
			\If{$i = n1$}
			{
				\For{$m \longleftarrow j$ \To $n2-1$}
				{
					$A[k] \longleftarrow R[m]$\;
					$k \longleftarrow k+1$\;
				}
			}

			\If{$j = j1$}
			{
				\For{$m \longleftarrow j$ \To $n1-1$}
				{
					$A[k] \longleftarrow L[m]$\;
					$k \longleftarrow k+1$\;
				}
			}
			\caption{MergeSort}
		\end{algorithm}
		
	\item[\textbf{Ex 2.3-3}]
		\textbf{Use mathematical induction to show that when $n$ is an exact power of 2, the solution of the recurrence\\
		\[ T(n) = \begin{cases} 2 & \quad \text{if $n$ = 2}\\ 
			2T(n/2)+n & \quad \text{if $n$ = $2^k$, for $k>1$} \end{cases} \] \\
			is $T(n) = n \lg n$.
			}
			
	\item[\textbf{Ex 2.3-4}]
		\textbf{We can express insertion sort as a recursive procedure as follows. In order to sort $A[1 \dots n]$, we recursively sort $A[1 \dots n-1]$ and then insert $A[n]$ into the sorted array $A[1 \dots n-1]$. Write a recurrence for the running time of this recursive version of insertion sort.}
			
	\item[A.]
		Let $T(n)$ be running time for insertion sort on an array of size $n$.\\
		\[ T(n) = \begin{cases} \Theta(1) & \quad \text{ if $n \leq c$}\\
		T(n-1) + I(n) & \quad \text{otherwise}
		\end{cases} \]
		where $I(n)$ denotes the amount of time taken to insert $A[n]$ into the sorted array $A[1 \dots n-1]$. Since we have to shift as many as $n-1$ elements once we find the correct place to insert $A[n]$, we have $I(n) = \Theta(n)$.
			
	\item[\textbf{Ex 2.3-5}]
		\textbf{If the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\Theta(\lg n)$.}
				
	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Sorted Array\\
			a $\longleftarrow$ start index\\
			b $\longleftarrow$ end index\\
			v $\longleftarrow$ value to be searched}
			\Output{i $\longleftarrow$ index of the value if found, else NIL}
			
			\DontPrintSemicolon
			\If{$a>b$}
			{
				\Return $NIL$\;
			}
			$m \longleftarrow \lfloor \frac{a+b}{2} \rfloor$\;
			\If{$A[m] = v$}
			{
				\Return $m$\;
			}
			\If{$A[m] < v$}
			{
				\Return RecursiveBinarySearch(a, m, v)\;
			}
			\Return RecursiveBinarySearch(m+1,b,v)\;
			\caption{RecursiveBinarySearch}
		\end{algorithm}
		
		After the initial of RecursiveBinarySearch(A,0,n,v), each call results a constant number of operations and a call to a problem instance where $b-a$ is a factor of $\frac{1}{2}$. So the recurrence relation satisfies $T(n) = T(n/2) + c$. So, $T(n) \in \Theta(\lg(n))$.
		
	\item[\textbf{Ex 2.3-6}]
		\textbf{Observe that the while loop of lines 5-7 of the INSERTION-SORT procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[1 \dots j-1]$. Can we use a binary search instead to improve the overall worst-case running time of insertion sort to $\Theta(n \lg n)$?}
			
	\item[\textbf{Ex 2.3-7}]
		\textbf{Describe a $\Theta(n \lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$.}
			
	\item[A.]
		Use Merge Sort to sort the array $S$ in time $\Theta(n \lg n)$.\\
		\begin{algorithm}[H]
			\Input{S $\longleftarrow$ Array of $n$ integers\\
			x $\longleftarrow$ sum to be found}
			\Output{If found return $true$, else $false$}
			
			\DontPrintSemicolon			
			
			$i \longleftarrow 0$\;
			$j \longleftarrow n$\;
			\While{$i<j$}
			{
				\If{$S[i] + S[j] = x$}
				{
					\Return $true$\;
				}
				\If{$S[i] + S[j] < x$}
				{
					$i \longleftarrow i+1$\;
				}
				\If{$S[i] + S[j] > x$}
				{
					$j \longleftarrow j-1$\;
				}
			}
			\Return $false$\;			
						
			\caption{FindSum}
		\end{algorithm}
	
\end{enumerate}

\subsection*{Problems}
\begin{enumerate}
	\item[\textbf{2-1}]
		\textbf{\textit{Insertion sort on small arrays in merge sort}\\
		Although merge sort runs in $\Theta(n \lg n)$ worst-case time and insertion sort runs in $\Theta(n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus, it makes sense to
coarsen the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in
which $n/k$ sublists of length $k$ are sorted using insertion sort and then merged
using the standard merging mechanism, where $k$ is a value to be determined.
		}
		\begin{enumerate}
			\item \textbf{Show that insertion sort can sort the $n/k$ sublists, each of length $k$, in $\Theta(nk)$ worst-case time.}
			\item[A.]
			Time for insertion sort to sort a single list of length $k$ is $\Theta(k^2)$, so $n/k$ of them will take $\Theta(\frac{n}{k} k^2)$ = $\Theta(nk)$.			
			
			\item \textbf{Show how to merge the sublists in $\Theta(n \lg(n/k))$ worst-case time.}
			\item[A.]
			Provided coarseness $k$, we can start usual merging procedure starting at the level in which array has a size at most $k$. So the depth of merge recursion tree is $\lg(n)-\lg(k)$ = $lg(n/k)$. Each level of merging is $cn$, so the total merging takes $\Theta(n \lg(n/k))$.			
			
			\item \textbf{Given that the modified algorithm runs in $\Theta(nk + n \lg(n/k))$ worst-case time, what is the largest value of $k$ as a function of $n$ for which the modified algorithm has the same running time as standard merge sort, in terms of $\Theta$-notation?}
			\item[A.]
			Considering $k$ as a function of $n$, $k(n)\in O(\log(n))$, gives the same asymptotics and for any constant choice of $k$, the asymptotics are the same.
			
			\item \textbf{How should we choose $k$ in practice?}
			\item[A.]
			We optimize the expression to get $c_1n - n(c_2)$ = 0 where $c_1$ and $c_2$ are coefficients of $nk$ and $n \lg(n/k)$. A constant choice of $k$ is optimal, in particular.
		\end{enumerate}	
		
	\item[\textbf{2-2}]
	\textbf{\textit{Correctness of bubblesort}\\
	Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly
swapping adjacent elements that are out of order.}
	
	\begin{enumerate}
		\item \textbf{Let $A'$ denote the output of BUBBLESORT(A). To prove that BUBBLESORT is correct, we need to prove that it terminates and that
		\begin{align}
		A'[1] \leq A'[2] \leq \dots \leq A'[n],		\label{2.3}
		\end{align}
		where $n=A.length$. In order to show that BUBBLESORT actually sorts, what else do we need to prove?\\
		The next two parts will prove inequality (2.3).}
		\item[A.]
		We need to prove that A 0 contains the same elements as A, which is easily seen to be true because the only modification we make to A is swapping its elements, so the resulting array must contain a rearrangement of the elements in the original array.
		
		\item \textbf{State precisely a loop invariant for the for loop in lines 2–4, and prove that this loop invariant holds. Your proof should use the structure of the loop invariant proof presented in this chapter.}
		\item[A.]
		At the start of each iteration, the position of the smallest element of
$A[i \dots n]$ is at most $j$, it’s true prior to first iteration where the position of any element is at most $A.length$. To see that each iteration maintains the loop invariant, suppose that $j = k$ and the position of the smallest element of $A[i \dots n]$ is at most $k$, then we compare $A[k]$ to $A[k-1]$. If $A[k] < A[k-1]$ then $A[k-1]$ is not the smallest element of $A[i \dots n]$, so when we swap $A[k]$ and $A[k-1]$ we know that the smallest element of $A[i \dots n]$ must occur in the first $k-1$ positions of the subarray, the maintaining the invariant. On the other hand, if $A[k] \geq A[k-1]$ then the smallest element can’t be $A[k]$. Since we do nothing, we conclude that the smallest element has position at most $k-1$. Upon termination, the smallest element of $A[i \dots n]$ is in position $i$.
		
		\item \textbf{Using the termination condition of the loop invariant proved in part(b), state a loop invariant for the for loop in lines 1–4 that will allow you to prove in-equality(2.3). Your proof should use the structure of the loop invariant proof
presented in this chapter.}
		\item[A.]
		At the start of each iteration the subarray $A[1..i-1]$ contains the $i-1$ smallest elements of $A$ in sorted order. Prior to the first iteration $i = 1$, and the first 0 elements of $A$ are trivially sorted. To see that each iteration maintains the loop invariant, fixing $i$ and suppose that $A[1 \dots i - 1]$ contains the $i - 1$ smallest elements of $A$ in sorted order. Then we run the loop in lines 2 through 4. We showed in part b that when this loop terminates, the smallest element of $A[i \dots n]$ is in position $i$. Since the $i-1$ smallest elements of $A$ are already in $A[1 \dots i - 1]$, $A[i]$ must be the $i$th smallest element of $A$. Therefore $A[1 \dots i]$ contains the $i$ smallest elements of $A$ in sorted order, maintaining the loop invariant. Upon termination, $A[1 \dots n]$ contains the $n$ elements of $A$ in sorted order as desired.

		\item \textbf{What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort?}
		\item[A.]
		The $i$th iteration of the for loop of lines 1 through 4 will cause $n-i$ iterations of the for loop of lines 2 through 4, each with constant time execution so the worst-case running time is $\Theta(n^2)$. 
This is the same as insertion sort; however bubble sort also has best-case running time $\Theta(n^2)$ whereas insertion sort has best-case running time $\Theta(n)$.		
	\end{enumerate}
	
	\item[\textbf{2-3}]
		\textbf{\textit{Correctness of Horner's rule}\\
		The following code fragment implements Horner's rule for evaluating a polynomial\\
		\begin{equation*}
			\begin{split}
 				P(x) & = \sum_{k=0}^{n} a_k x^k \\
 				& = a_0 + x (a_1 + x(a_2 + \dots + x(a_{n-1} + xa_n)\dots))
 			\end{split}
		\end{equation*}
		given the coefficients $a_0, a_1, \dots ,a_n$  and a value for $x$:
		\begin{algorithm}
			\SetAlgoNoEnd
			\SetAlgoNoLine
			$y \longleftarrow 0$\;
			\For{$i=n$ \DownTo $0$}
			{
				$y_i = a_i + x . y$
			}
		\end{algorithm}
		}
	\begin{enumerate}
		\item \textbf{In terms of $\Theta$-notation, what is the running time of this code fragment for Horner’s rule?}
		\item[A.]
		Assuming the arithmetic function is executed in constant time, then since the loop is being executed $n$ times, it has runtime $\Theta(n)$.
		
		\item \textbf{Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare to Horner’s rule?}
		\item[A.]
		\begin{algorithm}
			\SetAlgoNoEnd
			\SetAlgoNoLine
			$y \longleftarrow 0$\;
			\For{$i \longleftarrow 0$ \To $n$}
			{
				$y_i \longleftarrow 1$\;
				\For{$j \longleftarrow 1$ \To $i$}
				{
					$y_i \longleftarrow y_i * x$\;
				}
				$y = y + a_i . y_i$\;
			}			
		\end{algorithm}		
		The code has runtime $\Theta(n^2)$ as it has two nested for loops each running in linear time. It’s slower than Horner’s rule.
		
		\item \textbf{Consider the following loop invariant:\\
		At the start of each iteration of the for loop of lines 2–3,
		\begin{equation*}
		y = \sum_{k=0}^{n-(i+1)} a_{k+i+1} x^k
		\end{equation*}
		Interpret a summation with no terms as equalling 0. Following the structure of the loop invariant proof presented in this chapter, use this loop invariant to show that, at termination, $\sum_{k=0}^{n} a_k x^k$.
		}
		\item[A.]
		Initially $i=n$, so the upper bound of the summation is -1, so the sum evaluates to 0, which is the value of $y$. Assume that it is true for an $i$, then 
		\begin{equation*}
			\begin{split}
				y & = a_i + x \sum_{k=0}^{n (i+1)} a_{k+i+1} x^k \\
				& = a_i + x \sum_{k=1}^{n-i} a_{k+i} x^{k-1} \\
				& = \sum_{k=0}^{n-i} a_{k+i} x^{k}
			\end{split}
		\end{equation*}
		
		\item \textbf{Conclude by arguing that the given code fragment correctly evaluates a polynomial characterized by the coefficients $a_0, a_1, \dots , a_n$.}
		\item[A.]
		As stated in the previous problem, we evaluated the algorithm $\sum_{k=0}^{n} a_k x^k$ and the value of the polynomial evaluated at $x$.		
	\end{enumerate}
	
	\item[\textbf{2-4}]
	\textbf{\textit{Inversions}\\
Let $A[1 \dots n]$ be an array of n distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair($i$, $j$) is called an inversion of $A$.}
	\begin{enumerate}
		\item \textbf{List the five inversions of the array $(2, 3, 8, 6, 1)$}
		\item[A.]
		The five inversions are (2, 1), (3, 1), (8, 6), (8, 1), and (6, 1).
		
		\item \textbf{What array with elements from the set ${1, 2,\dots,n}$ has the most inversions? How many does it have?}
		\item[A.]
		The n-element array with the most inversions is $(n, n-1, \dots , 2, 1)$. It has $n-1+n-2+...+2+1 = n(n-1)/2$ inversions.
		
		\item \textbf{What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.}
		\item[A.]
		The running time is a constant times the no of inversions. Let $I(i)$ denote the number of $j < i$ such that $A[j] > A[i]$, and $\sum_{n}^{i=1} I(i)$ equals the number of inversions in $A$. Considering the while loop in the insertion sort algorithm, the loop will execute once for each element of A which has index less than j is larger than $A[j]$. Thus, it will execute $I(j)$ times. We reach the while loop once for each iteration in the for loop, so the no of constant time steps of insertion sort is  $\sum_{n}^{i=1} I(i)$ times of the inversion number of $A$.
		
		\item \textbf{Give an algorithm that determines the number of inversions in any permutation on $n$ elements in $\Theta(n lg n)$ worst-case time. (Hint: Modify merge sort)}
		\item[A.]
		\begin{algorithm}[H]
		\Input{A $\longleftarrow$ Unsorted array\\
		p $\longleftarrow$ start index\\
		r $\longleftarrow$ end index}
		\Output{inv $\longleftarrow$ No of inversions in the array}
		
		\If{$p<r$}
		{
			$q \longleftarrow \lfloor (p+r)/2 \rfloor $\;
			$left \longleftarrow$ Inversions($A, p, q$)\;
			$right \longleftarrow$ Inversions($A, q+1, r$)\;
			$inv \longleftarrow$ CountInversions($A, p, q, r$)$ + left + right$\;
			\Return $inv$\;
		}
		\Return 0\;
		\caption{Inversions}
		\end{algorithm}
		
		\begin{algorithm}[H]
		\Input{A $\longleftarrow$ Unsorted array\\
		p $\longleftarrow$ start index\\
		q $\longleftarrow$ middle index\\
		r $\longleftarrow$ end index}
		\Output{inv $\longleftarrow$ No of inversions in the array}
		
		$inv \longleftarrow 0$\;
		$n1 \longleftarrow q-p+1$\;
		$n2 \longleftarrow r-q$\;
		let $L[1,\dots , n1]$ and $R[1,\dots , n2]$ be new arrays\\
		\For{$i \longleftarrow 0$ \To $n1 - 1$}
		{
			$L[i] \longleftarrow A[p+i]$\;
		}
		\For{$j \longleftarrow 0$ \To $n2 - 1$}
		{
			$R[j] \longleftarrow A[q+j+1]$\;
		}
		$i \longleftarrow 0$\;
		$j \longleftarrow 0$\;		
		$k \longleftarrow p$\;
		\While{$i \neq n1$ \And $j \neq n2$}
		{
			\If{$L[i] \leq R[j]$}
			{
				$A[k] \longleftarrow L[i]$\;
				$i \longleftarrow i+1$\;
			}
			\Else
			{
				$inv \longleftarrow inv+j$ \tcc*{This keeps track of the number of inversions between the left and right arrays}\;
				$j \longleftarrow j+1$\;
			}
			$k \longleftarrow k+1$;
		}
		\If{$i = n1$}
		{
			\For{$m \longleftarrow j$ \To $n2-1$}
			{
				$A[k] \longleftarrow R[m]$\;
				$k \longleftarrow k+1$\;
			}
		}
		\If{$j = n2$}
		{
			\For{$m \longleftarrow i$ \To $n1-1$}
			{
				$A[k] \longleftarrow L[m]$\;
				$inv \longleftarrow inv+n2$\; \tcc*{Tracks inversions once we have exhausted the right array. At this point, every element of the right array contributes an inversion}
				$k \longleftarrow k+1$\;
			}
		}
		\Return $inv$\;
		\caption{CountInversions}
		\end{algorithm}
		
	\end{enumerate}


\end{enumerate}

\chapter{Growth of Functions}

\end{document}
