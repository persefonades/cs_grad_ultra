% cormen_solution.tex
\documentclass[fontsize=12pt,paper=a4,open=any]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx} 
% packages for math functions
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{listings}
% packages for algorithm
% \usepackage[noend]{algpseudocode}
\usepackage[linesnumbered,ruled]{algorithm2e}
% define variables for algorithm 
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Break}{break}
\SetKw{To}{to}
\SetKw{DownTo}{downto}
\SetKw{And}{and}
\SetKw{Or}{or}
\SetKwFunction{Func}{Fn}
% algorithm settings
\DontPrintSemicolon

\begin{document}
\title{Introduction to Algorithms - Solutions \\
\large 3rd Edition}
\author{Purbayan Chowdhury}

\maketitle

\tableofcontents

\chapter{The Role of Algorithms in Computing}

\chapter{Getting Started}

\section{Insertion Sort}
\begin{enumerate}
	\item[\textbf{Ex 2.1-1}]

	\item[\textbf{Ex 2.1-2}]
		\textbf{Rewrite the INSERTION-SORT procedure to sort into non-increasing instead of non-decreasing order.} \

	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Unsorted\ Array}
			\Output{A $\longleftarrow$ Array\ Sorted\ in\ Non-increasing Order}

			\For{$j \longleftarrow 1$ \To $A.length-1$}
			{
				$key\longleftarrow A[j]$\;
				\tcc{Insert A[j] into the sorted sequence A[1..j-1]}
				$i \longleftarrow j-1$\;
				\While{$i \ge 0$ \And $A[i] > key$}
				{
					$A[i+ 1] \longleftarrow A[i]$\;
					$i \longleftarrow i-1$\;
				}
				$A[i+ 1] \longleftarrow key$\;
			}

			\caption{Non-increasingInsertionSort}
		\end{algorithm}
	\item[\textbf{Ex 2.1-3}]
	
	\item[\textbf{Ex 2.1-4}]
		\textbf{Consider the searching problem:\
			Input: A sequence of n numbers $A =(a_1, a_2,\dots,a_n)$\
			Output: An index $i$ such that $v=A[i]$ or the special value NIL if $v$ does not appear in $A$.\
			Write pseudocode for linear search, which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfils the three necessary properties.
		}
	
	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Array\\
			v $\longleftarrow$ value to be searched}
			\Output{i $\longleftarrow$ index of the value if found, else NIL}

			$i \longleftarrow NIL$\;
			\For{$j \longleftarrow 0$ \To $A.length-1$}
			{
				\If{$A[j]=v$}
				{
					$i \longleftarrow j$\;
					\Break\;
				}
			}
			\Return $i$

			\caption{Linear-Search}
		\end{algorithm}

	\item[\textbf{Ex. 2.1-5}]
		\textbf{Consider the problem of adding two $n$-bit binary integers, stored in two n-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an ($n+1$)-element array $C$. State the problem formally and write pseudocode for adding the two integers.}

	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ First Array\\
			B $\longleftarrow$ Second Array}
			\Output{C $\longleftarrow$ Binary Addition Result}

			$carry \longleftarrow 0$\;
			\For{$i \longleftarrow n-1$ \DownTo $0$}
			{
				$C[i+1] \longleftarrow (A[i] + B[i]+carry) (\bmod 2)$\;
				\If{$A[i] + B[i] + carry \ge 2$}
				{
					$carry \longleftarrow 1$\;
				}
				\Else
				{
					$carry \longleftarrow 0$\;
				}
			}
			$C[0] \longleftarrow carry$\;
			\caption{$n$-bitBinaryAddition}
		\end{algorithm}

\end{enumerate}

\section{Analyzing algorithms}
\begin{enumerate}
	\item[\textbf{Ex 2.2-1}]

	\item[\textbf{Ex 2.2-2}]
		\textbf{Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n-1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort. What loop invariant does this algorithm maintain? Why does it need to run for only the first $n-1$ elements, rather than for all n elements? Give the best-case and worst-case running times of selection sort in $\Theta$-notation.}

	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Unsorted Array}
			\Output{A $\longleftarrow$ Array Sorted in Increasing Order}

			\DontPrintSemicolon

			\For{$i \longleftarrow 0$ \To $n-1$}
			{
				$min \longleftarrow i$\;
				\For{$j \longleftarrow i+1$ \To $n$}
				{
					\tcc{Find the index of the ith smallest element}
					\If{$A[j] < A[min]$}
					{
						$min \longleftarrow j$\;
					}
				}
				Swap $A[min]$ and $A[i]$\;
			}
			\caption{SelectionSort}
		\end{algorithm}

			The loop invariant of selection sort is as follows:\\
			At each iteration of the for loop of lines 1 through 9, the subarray $A[0 \dots i-1]$ contains the $i-1$ smallest elements of $A$ in increasing order. After $n-1$ iterations of the loop, the $n-1$ smallest elements of $A$ are in the first $n-1$ positions of $A$ in increasing order so the nth element is necessarily the largest amount.\\
			The best-case and worst-case running times of selection sort are $\Theta(n^2)$, this is because regardless of how the elements are initially arranged, on the $i$-th iteration of the for loop in line 1, always inspects each of the remaining $n-i$ elements to find the smallest one remaining. \\
			This yields a running \\
			$\sum_{i=1}^{n-1} n-i = n(n-1) - \sum_{i=1}^{n-1} i = n^2-n-\frac{n^2-n}{2} = \frac{n^2-n}{2} = \Theta(n^2)$

		\item[\textbf{Ex 2.2-3}]

		\item[\textbf{Ex 2.2-4}]
\end{enumerate}

\section{Designing algorithms}
\begin{enumerate}
	\item[\textbf{Ex 2.3.1}]
		\textbf{Illustrate the operation of merge sort on the array A=(3, 41, 52, 26, 38, 57, 9, 49)}

	\item[\textbf{Ex 2.3.2}]
		\textbf{Rewrite the MERGE procedure so that it does not use sentinels, instead stopping once either array L or R has had all its elements copied back to A and then copying the remainder of the other array back into A.}

	\item[A.]
		\begin{algorithm}[H]
			\SetKwInOut{Input}{Input}
			\SetKwInOut{Output}{Output}
			\Input{A $\longleftarrow$ Unsorted Array\\
			p $\longleftarrow$ start index\\
			q $\longleftarrow$ middle index\\
			r $\longleftarrow$ end index}
			\Output{A $\longleftarrow$ Array Sorted in Increasing Order}

			$n1 \longleftarrow q-p+1$\;
			$n2 \longleftarrow r-q$\;
			let $L[1,\dots,n1]$ and $R[1,\dots,n2]$ be new arrays\;
			\For{$i \longleftarrow 0$ \To $n1-1$}
			{
				$L[i] \longleftarrow A[p+i]$\;
			}
			\For{$j \longleftarrow 0$ \To $n2-1$}
			{
				$R[j] \longleftarrow A[q+j+i]$\;
			}

			$i \longleftarrow 0$\;
			$j \longleftarrow 0$\;
			$k \longleftarrow p$\;

			\While{$i \neq n1$ \And $j \neq n2$}
			{
				\If{$L[i] \leq R[j]$}
				{
					$A[k] \longleftarrow L[i]$\;
					$i \longleftarrow i+1$\;
				}
				\Else
				{
					$A[k] \longleftarrow R[j]$\;
					$j \longleftarrow j+1$\;
				}
				$k \longleftarrow k+1$\;
			}
			
			\If{$i = n1$}
			{
				\For{$m \longleftarrow j$ \To $n2-1$}
				{
					$A[k] \longleftarrow R[m]$\;
					$k \longleftarrow k+1$\;
				}
			}

			\If{$j = j1$}
			{
				\For{$m \longleftarrow j$ \To $n1-1$}
				{
					$A[k] \longleftarrow L[m]$\;
					$k \longleftarrow k+1$\;
				}
			}
			\caption{MergeSort}
		\end{algorithm}
		
	\item[\textbf{Ex 2.3-3}]
		\textbf{Use mathematical induction to show that when $n$ is an exact power of 2, the solution of the recurrence\\
		\[ T(n) = \begin{cases} 2 & \quad \text{if $n$ = 2}\\ 
			2T(n/2)+n & \quad \text{if $n$ = $2^k$, for $k>1$} \end{cases} \] \\
			is $T(n) = n \lg n$.
			}
			
	\item[\textbf{Ex 2.3-4}]
		\textbf{We can express insertion sort as a recursive procedure as follows. In order to sort $A[1 \dots n]$, we recursively sort $A[1 \dots n-1]$ and then insert $A[n]$ into the sorted array $A[1 \dots n-1]$. Write a recurrence for the running time of this recursive version of insertion sort.}
			
	\item[A.]
		Let $T(n)$ be running time for insertion sort on an array of size $n$.\\
		\[ T(n) = \begin{cases} \Theta(1) & \quad \text{ if $n \leq c$}\\
		T(n-1) + I(n) & \quad \text{otherwise}
		\end{cases} \]
		where $I(n)$ denotes the amount of time taken to insert $A[n]$ into the sorted array $A[1 \dots n-1]$. Since we have to shift as many as $n-1$ elements once we find the correct place to insert $A[n]$, we have $I(n) = \Theta(n)$.
			
	\item[\textbf{Ex 2.3-5}]
		\textbf{If the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\Theta(\lg n)$.}
				
	\item[A.]
		\begin{algorithm}[H]
			\Input{A $\longleftarrow$ Sorted Array\\
			a $\longleftarrow$ start index\\
			b $\longleftarrow$ end index\\
			v $\longleftarrow$ value to be searched}
			\Output{i $\longleftarrow$ index of the value if found, else NIL}
			
			\DontPrintSemicolon
			\If{$a>b$}
			{
				\Return $NIL$\;
			}
			$m \longleftarrow \lfloor \frac{a+b}{2} \rfloor$\;
			\If{$A[m] = v$}
			{
				\Return $m$\;
			}
			\If{$A[m] < v$}
			{
				\Return RecursiveBinarySearch(a, m, v)\;
			}
			\Return RecursiveBinarySearch(m+1,b,v)\;
			\caption{RecursiveBinarySearch}
		\end{algorithm}
		
		After the initial of RecursiveBinarySearch(A,0,n,v), each call results a constant number of operations and a call to a problem instance where $b-a$ is a factor of $\frac{1}{2}$. So the recurrence relation satisfies $T(n) = T(n/2) + c$. So, $T(n) \in \Theta(\lg(n))$.
		
	\item[\textbf{Ex 2.3-6}]
		\textbf{Observe that the while loop of lines 5-7 of the INSERTION-SORT procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[1 \dots j-1]$. Can we use a binary search instead to improve the overall worst-case running time of insertion sort to $\Theta(n \lg n)$?}
			
	\item[\textbf{Ex 2.3-7}]
		\textbf{Describe a $\Theta(n \lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$.}
			
	\item[A.]
		Use Merge Sort to sort the array $S$ in time $\Theta(n \lg n)$.\\
		\begin{algorithm}[H]
			\Input{S $\longleftarrow$ Array of $n$ integers\\
			x $\longleftarrow$ sum to be found}
			\Output{If found return $true$, else $false$}
			
			\DontPrintSemicolon			
			
			$i \longleftarrow 0$\;
			$j \longleftarrow n$\;
			\While{$i<j$}
			{
				\If{$S[i] + S[j] = x$}
				{
					\Return $true$\;
				}
				\If{$S[i] + S[j] < x$}
				{
					$i \longleftarrow i+1$\;
				}
				\If{$S[i] + S[j] > x$}
				{
					$j \longleftarrow j-1$\;
				}
			}
			\Return $false$\;			
						
			\caption{FindSum}
		\end{algorithm}
	
\end{enumerate}

\subsection*{Problems}
\begin{enumerate}
	\item[\textbf{2-1}]
		\textbf{\textit{Insertion sort on small arrays in merge sort}\\
		Although merge sort runs in $\Theta(n \lg n)$ worst-case time and insertion sort runs in $\Theta(n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus, it makes sense to
coarsen the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in
which $n/k$ sublists of length $k$ are sorted using insertion sort and then merged
using the standard merging mechanism, where $k$ is a value to be determined.
		}
		\begin{enumerate}
			\item \textbf{Show that insertion sort can sort the $n/k$ sublists, each of length $k$, in $\Theta(nk)$ worst-case time.}
			\item[A.]
			Time for insertion sort to sort a single list of length $k$ is $\Theta(k^2)$, so $n/k$ of them will take $\Theta(\frac{n}{k} k^2)$ = $\Theta(nk)$.			
			
			\item \textbf{Show how to merge the sublists in $\Theta(n \lg(n/k))$ worst-case time.}
			\item[A.]
			Provided coarseness $k$, we can start usual merging procedure starting at the level in which array has a size at most $k$. So the depth of merge recursion tree is $\lg(n)-\lg(k)$ = $lg(n/k)$. Each level of merging is $cn$, so the total merging takes $\Theta(n \lg(n/k))$.			
			
			\item \textbf{Given that the modified algorithm runs in $\Theta(nk + n \lg(n/k))$ worst-case time, what is the largest value of $k$ as a function of $n$ for which the modified algorithm has the same running time as standard merge sort, in terms of $\Theta$-notation?}
			\item[A.]
			Considering $k$ as a function of $n$, $k(n)\in O(\log(n))$, gives the same asymptotics and for any constant choice of $k$, the asymptotics are the same.
			
			\item \textbf{How should we choose $k$ in practice?}
			\item[A.]
			We optimize the expression to get $c_1n - n(c_2)$ = 0 where $c_1$ and $c_2$ are coefficients of $nk$ and $n \lg(n/k)$. A constant choice of $k$ is optimal, in particular.
		\end{enumerate}	
		
	\item[\textbf{2-2}]
	\textbf{\textit{Correctness of bubblesort}\\
	Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly
swapping adjacent elements that are out of order.}
	
	\begin{enumerate}
		\item \textbf{Let $A'$ denote the output of BUBBLESORT(A). To prove that BUBBLESORT is correct, we need to prove that it terminates and that
		\begin{align}
		A'[1] \leq A'[2] \leq \dots \leq A'[n],		\label{2.3}
		\end{align}
		where $n=A.length$. In order to show that BUBBLESORT actually sorts, what else do we need to prove?\\
		The next two parts will prove inequality (2.3).}
		\item[A.]
		We need to prove that A 0 contains the same elements as A, which is easily seen to be true because the only modification we make to A is swapping its elements, so the resulting array must contain a rearrangement of the elements in the original array.
		
		\item \textbf{State precisely a loop invariant for the for loop in lines 2–4, and prove that this loop invariant holds. Your proof should use the structure of the loop invariant proof presented in this chapter.}
		\item[A.]
		At the start of each iteration, the position of the smallest element of
$A[i \dots n]$ is at most $j$, it’s true prior to first iteration where the position of any element is at most $A.length$. To see that each iteration maintains the loop invariant, suppose that $j = k$ and the position of the smallest element of $A[i \dots n]$ is at most $k$, then we compare $A[k]$ to $A[k-1]$. If $A[k] < A[k-1]$ then $A[k-1]$ is not the smallest element of $A[i \dots n]$, so when we swap $A[k]$ and $A[k-1]$ we know that the smallest element of $A[i \dots n]$ must occur in the first $k-1$ positions of the subarray, the maintaining the invariant. On the other hand, if $A[k] \geq A[k-1]$ then the smallest element can’t be $A[k]$. Since we do nothing, we conclude that the smallest element has position at most $k-1$. Upon termination, the smallest element of $A[i \dots n]$ is in position $i$.
		
		\item \textbf{Using the termination condition of the loop invariant proved in part(b), state a loop invariant for the for loop in lines 1–4 that will allow you to prove in-equality(2.3). Your proof should use the structure of the loop invariant proof
presented in this chapter.}
		\item[A.]
		At the start of each iteration the subarray $A[1..i-1]$ contains the $i-1$ smallest elements of $A$ in sorted order. Prior to the first iteration $i = 1$, and the first 0 elements of $A$ are trivially sorted. To see that each iteration maintains the loop invariant, fixing $i$ and suppose that $A[1 \dots i - 1]$ contains the $i - 1$ smallest elements of $A$ in sorted order. Then we run the loop in lines 2 through 4. We showed in part b that when this loop terminates, the smallest element of $A[i \dots n]$ is in position $i$. Since the $i-1$ smallest elements of $A$ are already in $A[1 \dots i - 1]$, $A[i]$ must be the $i$th smallest element of $A$. Therefore $A[1 \dots i]$ contains the $i$ smallest elements of $A$ in sorted order, maintaining the loop invariant. Upon termination, $A[1 \dots n]$ contains the $n$ elements of $A$ in sorted order as desired.

		\item \textbf{What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort?}
		\item[A.]
		The $i$th iteration of the for loop of lines 1 through 4 will cause $n-i$ iterations of the for loop of lines 2 through 4, each with constant time execution so the worst-case running time is $\Theta(n^2)$. 
This is the same as insertion sort; however bubble sort also has best-case running time $\Theta(n^2)$ whereas insertion sort has best-case running time $\Theta(n)$.		
	\end{enumerate}
	
	\item[\textbf{2-3}]
		\textbf{\textit{Correctness of Horner's rule}\\
		The following code fragment implements Horner's rule for evaluating a polynomial\\
		\begin{equation*}
			\begin{split}
 				P(x) & = \sum_{k=0}^{n} a_k x^k \\
 				& = a_0 + x (a_1 + x(a_2 + \dots + x(a_{n-1} + xa_n)\dots))
 			\end{split}
		\end{equation*}
		given the coefficients $a_0, a_1, \dots ,a_n$  and a value for $x$:
		\begin{algorithm}
			\SetAlgoNoEnd
			\SetAlgoNoLine
			$y \longleftarrow 0$\;
			\For{$i=n$ \DownTo $0$}
			{
				$y_i = a_i + x . y$
			}
		\end{algorithm}
		}
	\begin{enumerate}
		\item \textbf{In terms of $\Theta$-notation, what is the running time of this code fragment for Horner’s rule?}
		\item[A.]
		Assuming the arithmetic function is executed in constant time, then since the loop is being executed $n$ times, it has runtime $\Theta(n)$.
		
		\item \textbf{Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare to Horner’s rule?}
		\item[A.]
		\begin{algorithm}
			\SetAlgoNoEnd
			\SetAlgoNoLine
			$y \longleftarrow 0$\;
			\For{$i \longleftarrow 0$ \To $n$}
			{
				$y_i \longleftarrow 1$\;
				\For{$j \longleftarrow 1$ \To $i$}
				{
					$y_i \longleftarrow y_i * x$\;
				}
				$y = y + a_i . y_i$\;
			}			
		\end{algorithm}		
		The code has runtime $\Theta(n^2)$ as it has two nested for loops each running in linear time. It’s slower than Horner’s rule.
		
		\item \textbf{Consider the following loop invariant:\\
		At the start of each iteration of the for loop of lines 2–3,
		\begin{equation*}
		y = \sum_{k=0}^{n-(i+1)} a_{k+i+1} x^k
		\end{equation*}
		Interpret a summation with no terms as equalling 0. Following the structure of the loop invariant proof presented in this chapter, use this loop invariant to show that, at termination, $\sum_{k=0}^{n} a_k x^k$.
		}
		\item[A.]
		Initially $i=n$, so the upper bound of the summation is -1, so the sum evaluates to 0, which is the value of $y$. Assume that it is true for an $i$, then 
		\begin{equation*}
			\begin{split}
				y & = a_i + x \sum_{k=0}^{n (i+1)} a_{k+i+1} x^k \\
				& = a_i + x \sum_{k=1}^{n-i} a_{k+i} x^{k-1} \\
				& = \sum_{k=0}^{n-i} a_{k+i} x^{k}
			\end{split}
		\end{equation*}
		
		\item \textbf{Conclude by arguing that the given code fragment correctly evaluates a polynomial characterized by the coefficients $a_0, a_1, \dots , a_n$.}
		\item[A.]
		As stated in the previous problem, we evaluated the algorithm $\sum_{k=0}^{n} a_k x^k$ and the value of the polynomial evaluated at $x$.		
	\end{enumerate}
	
	\item[\textbf{2-4}]
	\textbf{\textit{Inversions}\\
Let $A[1 \dots n]$ be an array of n distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair($i$, $j$) is called an inversion of $A$.}
	\begin{enumerate}
		\item \textbf{List the five inversions of the array $(2, 3, 8, 6, 1)$}
		\item[A.]
		The five inversions are (2, 1), (3, 1), (8, 6), (8, 1), and (6, 1).
		
		\item \textbf{What array with elements from the set ${1, 2,\dots,n}$ has the most inversions? How many does it have?}
		\item[A.]
		The n-element array with the most inversions is $(n, n-1, \dots , 2, 1)$. It has $n-1+n-2+...+2+1 = n(n-1)/2$ inversions.
		
		\item \textbf{What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.}
		\item[A.]
		The running time is a constant times the no of inversions. Let $I(i)$ denote the number of $j < i$ such that $A[j] > A[i]$, and $\sum_{n}^{i=1} I(i)$ equals the number of inversions in $A$. Considering the while loop in the insertion sort algorithm, the loop will execute once for each element of A which has index less than j is larger than $A[j]$. Thus, it will execute $I(j)$ times. We reach the while loop once for each iteration in the for loop, so the no of constant time steps of insertion sort is  $\sum_{n}^{i=1} I(i)$ times of the inversion number of $A$.
		
		\item \textbf{Give an algorithm that determines the number of inversions in any permutation on $n$ elements in $\Theta(n \lg n)$ worst-case time. (Hint: Modify merge sort)}
		\item[A.]
		\begin{algorithm}[H]
		\Input{A $\longleftarrow$ Unsorted array\\
		p $\longleftarrow$ start index\\
		r $\longleftarrow$ end index}
		\Output{inv $\longleftarrow$ No of inversions in the array}
		
		\If{$p<r$}
		{
			$q \longleftarrow \lfloor (p+r)/2 \rfloor $\;
			$left \longleftarrow$ Inversions($A, p, q$)\;
			$right \longleftarrow$ Inversions($A, q+1, r$)\;
			$inv \longleftarrow$ CountInversions($A, p, q, r$)$ + left + right$\;
			\Return $inv$\;
		}
		\Return 0\;
		\caption{Inversions}
		\end{algorithm}
		
		\begin{algorithm}[H]
		\Input{A $\longleftarrow$ Unsorted array\\
		p $\longleftarrow$ start index\\
		q $\longleftarrow$ middle index\\
		r $\longleftarrow$ end index}
		\Output{inv $\longleftarrow$ No of inversions in the array}
		
		$inv \longleftarrow 0$\;
		$n1 \longleftarrow q-p+1$\;
		$n2 \longleftarrow r-q$\;
		let $L[1,\dots , n1]$ and $R[1,\dots , n2]$ be new arrays\\
		\For{$i \longleftarrow 0$ \To $n1 - 1$}
		{
			$L[i] \longleftarrow A[p+i]$\;
		}
		\For{$j \longleftarrow 0$ \To $n2 - 1$}
		{
			$R[j] \longleftarrow A[q+j+1]$\;
		}
		$i \longleftarrow 0$\;
		$j \longleftarrow 0$\;		
		$k \longleftarrow p$\;
		\While{$i \neq n1$ \And $j \neq n2$}
		{
			\If{$L[i] \leq R[j]$}
			{
				$A[k] \longleftarrow L[i]$\;
				$i \longleftarrow i+1$\;
			}
			\Else
			{
				$inv \longleftarrow inv+j$ \tcc*{This keeps track of the number of inversions between the left and right arrays}\;
				$j \longleftarrow j+1$\;
			}
			$k \longleftarrow k+1$;
		}
		\If{$i = n1$}
		{
			\For{$m \longleftarrow j$ \To $n2-1$}
			{
				$A[k] \longleftarrow R[m]$\;
				$k \longleftarrow k+1$\;
			}
		}
		\If{$j = n2$}
		{
			\For{$m \longleftarrow i$ \To $n1-1$}
			{
				$A[k] \longleftarrow L[m]$\;
				$inv \longleftarrow inv+n2$\; \tcc*{Tracks inversions once we have exhausted the right array. At this point, every element of the right array contributes an inversion}
				$k \longleftarrow k+1$\;
			}
		}
		\Return $inv$\;
		\caption{CountInversions}
		\end{algorithm}
		
	\end{enumerate}
\end{enumerate}

\chapter{Growth of Functions}

\section{Asymptotic notation}

\begin{enumerate}
	\item[\textbf{Ex 3.1-1}]
		\textbf{Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\Theta$-notation, prove that max($f(n).g(n)$) = $\Theta(f(n)+g(n))$.}
	\item[A.]
	
	\item[\textbf{Ex 3.1-2}]
	\textbf{Show that for any real constants $a$ and $b$, where $b$ > 0,\\
	$(n+a)^b = \Theta(n^b)$}
	\item[A.]
	Let $c = 2^b$ and $n_0 \geq 2a$, then for all $n \geq n_0$, we have $(n+a)^b \leq (2n)^b = cn^b$, so $(n+a)^b = O(n^b)$. Let $n_0 \geq \frac{-a}{1-1/2^{1/b}}$ and $c = 1/2$, then $n \geq n_0 \geq \frac{-a}{1-1/2^{1/b}}$ if and only if $n-2^{1/b} \geq -a$, also $n+a \geq (1/2)^{a/b}n$, also $(n+a)^b \geq cn^b$. Therefore $(n+a)^b = \Omega(n^b)$. By Theorem 3.1, $(n+a)^b = \Theta(n^b)$.
	
	\item[\textbf{Ex 3.1-3}]
		\textbf{Explain why the statement, “The running time of algorithm $A$ is at least $O(n^2)$,” is meaningless.}
	\item[A.]
	
	\item[\textbf{Ex 3.1-4}]
		\textbf{Is $2^{n+1} = O(n^2)$? Is $2^{2n} = O(n^2)$?}
	\item[A.]
	$2^{n+1} \geq 2 . 2^n$ for all $n \geq 0$, so $2^{n+1} = O(2^n)$, but, $2^{2n}$ is not $O(2^n)$. If there would exist $n_0$ and $c$ such that $n \geq n_0$ implies $2^n . 2^n = 2^{2n} \leq c2^n,$ so $2^n \leq c$ for $\ n \geq n_0$, which is clearly impossible since $c$ is a constant.
	
	\item[\textbf{Ex 3.1-5}]
		\textbf{Prove Theorem 3.1}
	\item[A.]
	Suppose $f(n) \in \Theta(g(n))$, then $\exists c_1, c_2, n_0, \forall n \geq n_0, 0 \leq c_1g(n) \leq f(n) \leq c_2g(n)$. We have $c_1g(n) \leq f(n)(f(n) \in \Omega(g(n)))$ and $f(n) \leq c_2g(n)(f(n) \in O(g(n)))$.
Suppose that we had $\exists n_1, c_1, \forall n \geq n_1, c_1g(n) \leq f(n)$ and $\exists n_2,c_2,\forall n \geq n_2, f(n) \leq c_2g(n)$. Putting these together, letting $n_0 = max(n_1,n_2)$, we have $\forall n \geq n_0$, $c_1g(n) \leq f(n) \leq c_2g(n)$.

	\item[\textbf{Ex 3.1-6}]
	\textbf{Prove that the running time of an algorithm is $\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\Omega(g(n))$.}
	
	\item[\textbf{Ex 3.1-7}]
	\textbf{Prove that $o(g(n)) \cap \omega(g(n))$ is the empty set.}

	\item[\textbf{Ex 3.1-8}]
	\textbf{We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n, m)$, we denote by $O(g(n, m))$ the set of functions
	\begin{equation*}
		\begin{split}
			O(g(n, m)) = \{f(n,m) : & \text{ there exist positive constants} \\
			& \text{ such that } 0 \leq f(n,m) \leq cg(n,m) \\ 
			& \text{ for all } n \geq n_0 \text{ or } m \geq m_0 \}.
		\end{split}
	\end{equation*}
	Give corresponding definitions for $\Omega(g(n,m))$ and $\Theta(g(n,m))$.
	}	
\end{enumerate}

\section{Standard notations and common functions}

\begin{enumerate}
	\item[\textbf{Ex 3.2-1}]
		\textbf{Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n) + g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n) . g(n)$ is monotonically increasing.}
		
	\item[\textbf{Ex 3.2-2}]
		\textbf{Prove the equation (3.16).}
		
	\item[\textbf{Ex 3.2-3}]
		\textbf{Prove the equation (3.19). Also prove that $n! = \omega(2^n)$ and $n! = o(n^n)$.}
		
	\item[\textbf{Ex 3.2-4}]
		\textbf{Is the function $\lceil \lg n \rceil !$ polynomially bounded? Is the function $\lceil \lg \lg n \rceil !$ polynomially bounded?}
		
	\item[\textbf{Ex 3.2-5}]
		\textbf{Which is asymptotically larger: $\lg(\lg^* n)$ or $\lg^*(\lg n)$?}
		
	\item[\textbf{Ex 3.2-6}]
		\textbf{Show that the golden ratio $\phi$ and its conjugate $\hat{\phi}$ both satisfy the equation $x^2 = x + 1$.}
		
	\item[\textbf{Ex 3.2-7}]
		\textbf{Prove by induction that the $i$th Fibonacci number satisfies the equality \\
		$F_i = \frac{\phi^i - \hat{\phi^i}}{\sqrt{5}}$ \\
		where $\phi$ is the golden ratio and $\hat{\phi}$ is its conjugate.}
		
	\item[\textbf{Ex 3.2-8}]
		\textbf{Show that $k \ln k = \Theta(n)$ implies $k=\Theta (n/\ln n)$.}
\end{enumerate}

\subsection*{Problems}
\begin{enumerate}
	\item[\textbf{3-1}]
		\textbf{\textit{Asymptotic behavior of polynomials}\\
		Let \\
		$p(n) = \sum_{i=0}^{d} a_in^i$,\\
		where $a_d=0$, be a degree-$d$ polynomial in $n$, and let $k$ be a constant. Use the definitions of the asymptotic notations to prove the following properties.
		\begin{enumerate}
			\item If $k \geq d$, then $p(n) = O(n^k)$.
			\item If $k \leq d$, then $p(n) = \Omega(n^k)$.
			\item If $k = d$, then $p(n) = \Theta(n^k)$.
			\item If $k > d$, then $p(n) = o(n^k)$.
			\item If $k < d$, then $p(n) = \omega(n^k)$.
		\end{enumerate}
		}
	
	\item[\textbf{3-2}]
		\textbf{\textit{Relative asymptotic growths}\\
		Indicate, for each pair of expressions $(A, B)$ in the table below, whether $A$ is $O$, $o$, $\Omega$, $\omega$, or $\Theta$ of $B$. Assume that $k \geq 1$, $\epsilon \> 0$, and $c>1$ are constants. Your answer should be in the form of the table with “yes” or “no” written in each box.
		\begin{tabular}{c c | p{2em} | p{2em} | p{2em} | p{2em} | p{2em} |}
			$A$ & $B$ & $O$ & $o$ & $\Omega$ & $\omega$ & $\Theta$ \\
			\hline
			$\lg^k n$ & $n^\epsilon$ & & & & & \\	
			\hline
			$n^k n$ & $c^n$ & & & & & \\	
			\hline
			$\sqrt{n}$ & $n^{\sin n}$ & & & & & \\	
			\hline
			$2^n$ & $2^{n/2}$ & & & & & \\	
			\hline
			$n^{\lg c}$ & $c^{\lg n}$ & & & & & \\	
			\hline
			$\lg(n!) n$ & $\lg(n^n)$ & & & & & \\	
			\hline			
		\end{tabular}
		}
		
	\item[\textbf{3-3}]
		\textbf{\textit{Ordering by asymptotic growth rates}
			\begin{enumerate}
				\item Rank the following functions by order of growth; that is, find an arrangement $g_1, g_2, \dots , g_30$ of the functions satisfying $g_1 = \Omega (g_2), g_2 D \Omega (g_3), \dots,g_29 = \Omega (g_30)$s. Partition your list into equivalence classes such that functions $f(n)$ and $g(n)$ are in the same class if and only if $f(n) = \Theta(g(n))$.
				\begin{tabular}{c c c c c c}
				$\lg(\lg^* n)$ & $2^{lg^* n}$ & $(\sqrt{2})^{\lg n}$ & $n^2$ & $n!$ & $(\lg n)!$ \\				
				$(\frac{3}{2})^n$& $n^3$ & $\lg^2 n$ & $\lg(n!)$ & $2^{2^n}$ & $n^{1/\lg n}$ \\				
				$\ln \ln n$& $\lg^* n$ & $n . 2^n$ & $n^{\lg \lg n}$ & $\ln n$ & $1$ \\				
				$2^{\lg n}$& $(\lg n)^{\lg n}$ & $e^n$ & $4^{\lg n}$ & $(n+1)!$ & $\sqrt{\lg n}$ \\				
				$(\lg^*(\lg n)$& $2^{\sqrt{2 \lg n}}$ & $n$ & $2^n$ & $n \lg n$ & $2^{2^{n+1}}$ \\
				\end{tabular}				
				\item Give an example of a single nonnegative function $f(n)$ such that for all functions $g_i(n)$ in part (a), $f(n)$ is neither $O(g_i(n))$ nor $\Theta(g_i(n))$.				
			\end{enumerate}
		}
		
	\item[\textbf{3-4}]
		\textbf{\textit{Asymptotic notation properties}\\
			Let $f(n)$ and $g(n)$ be asymptotically positive functions. Prove or disprove each of the following conjectures.
			\begin{enumerate}
				\item $f(n) = O(g(n))$ implies $g(n) = O(f(n))$
				\item $f(n) + g(n) = \Theta(min(f(n), g(n)))$
				\item $f(n) = O(g(n))$ implies $(\lg(f(n)))$, where $\lg(g(n)) \geq 1$ and $f(n) \geq 1$ for all sufficiently large $n$.
				\item $f(n) = O(g(n))$ implies $2^{f(n)} = O(2^{g(n)})$.
				\item $f(n) = O((f(n))^2)$.
				\item $f(n) = O(g(n))$ implies $g(n) = \Omega(f(n))$.
				\item $f(n) = \Theta((f(n/2))$
				\item $f(n) + o(f(n)) = \Theta((f(n))$
			\end{enumerate}
		}
			
	\item[\textbf{3-5}]
		\textbf{\textit{Asymptotic notation properties}\\
			Some authors define $\Omega$ in a slightly different way than we do; let’s use $\Omega^\infty$ (read “omega infinity”) for this alternative definition. We say that $f(n) = \Omega^\infty (g(n))$ if there exists a positive constant $c$ such that $f(n) \geq cg(n) \geq 0$ for infinitely many integers $n$.
			\begin{enumerate}
				\item Show that for any two functions $f(n)$ and $g(n)$ that are asymptotically nonnegative, either $f(n) = O(g(n))$ or $f(n) = \Omega^\infty(g(n))$ or both, whereas this is not true if we use $\Omega$ in place of $\Omega^\infty$.
				\item Describe the potential advantages and disadvantages of using $\Omega^\infty$ instead of $\Omega$ to characterize the running times of programs.\\
				Some authors also define $O$ in a slightly different manner; let’s use $O'$ for the alternative definition. We say that $f(n) = O'(g(n))$ if and only if $\lvert f(n) \rvert = O(g(n))$.
				\item What happens to each direction of the “if and only if” in Theorem 3.1 if we substitute $O'$ for $O$ but still use $\Omega$?\\
				Some authors define $\tilde{O}$ (read “soft-oh”) to mean $O$ with logarithmic factors ignored:
				\begin{equation*}
					\begin{split}
						\tilde{O}g(n) = \{f(n) : & \text{ there exist positive constants  } c, k \text{ and } n_0 \text{ such that }\\
						& \ 0\leq f(n) \leq cg(n)\lg^k(n) \text{ for all } n \geq n_0 \}
					\end{split}
				\end{equation*}
				\item Define $\tilde{\Omega}$ and $\tilde{\Theta}$ in a similar manner. Prove the corresponding analog to Theorem 3.1.
			\end{enumerate}
		}
		
	\item[\textbf{3-6}]
		\textbf{\textit{Iterated functions}\\
			We can apply the iteration operator* used in the lg* function to any monotonically increasing function $f(n)$ over the reals. For a given constant $c \in \mathbb{R}$, we define the iterated function $f_c^*$ by\\
			\[ f_c^*(n) = \min\{ i \geq 0\ :\ f^{(i)} \leq c\},\] \\
			which need not be well defined in all cases. In other words, the quantity $f_c^*(n)$ is the number of iterated applications of the function $f$ required to reduce its argument down to $c$ or less.\\
			For each of the following functions $f(n)$ and constants $c$, give as tight a bound as possible on $f_c^*(n)$. \\
			\begin{tabular}{c c | c |}
				$f(n)$ & $c$ & $f_c^*(n)$ \\
				\hline
				$n-1$ & $0$ & $\lceil n \rceil$ \\
				\hline
				$\lg n$ & $1$ & $\lg^* n$ \\
				\hline
				$n/2$ & $1$ & $\lceil \lg(n) \rceil$ \\
				\hline
				$n/2$ & $2$ & $\lceil \lg(n) \rceil - 1$\\
				\hline
				$\sqrt{n}$ & $2$ & $\lg \lg n$\\ 
				\hline
				$\sqrt{n}$ & $1$ & undefined\\
				\hline
				$n^{1/3}$ & $2$ & $\lg_3 \lg_2 (n)$\\
				\hline
				$n/\lg n$ & $2$ & $\Omega(\frac{\lg n}{\lg \lg n})$\\
				\hline
			\end{tabular}		
			}
	
\end{enumerate}


\chapter{Divide-and-Conquer}

\section{The maximum-subarray problem}

\section{Strassen’s algorithm for matrix multiplication}

\section{The substitution method for solving recurrences}

\section{The recursion-tree method for solving recurrences}

\section{The master method for solving recurrences}

\section{Proof of the master theorem}

\subsection*{Problems}


\chapter{Probabilistic Analysis and Randomized Algorithms}

\section{The hiring problem}

\section{Indicator random variables}

\section{Randomized algorithms}

\section{Probabilistic analysis and further uses of indicator random variables}

\subsection*{Problems}


\chapter{Heapsort}

\section{Heaps}

\section{Maintaining the heap property}

\section{Building a heap}

\section{The heapsort algorithm}

\section{Priority queues}

\subsection*{Problems}


\chapter{Quicksort}

\section{Description of quicksort}

\section{Performance of quicksort}

\section{A randomized version of quicksort}

\section{Analysis of quicksort}

\subsection*{Problems}


\chapter{Sorting in Linear Time}

\section{Lower bounds for sorting}

\section{Counting sort}

\section{Radix sort}

\section{Bucket sort}

\subsection*{Problems}


\chapter{Medians and Order Statistics}

\section{Minimum and maximum}

\section{Selection in expected linear time}

\section{Selection in worst-case linear time}

\subsection*{Problems}


\chapter{Elementary Data Structures}

\section{Stacks and queues}

\section{Linked lists}

\section{Implementing pointers and objects}

\section{Representing rooted trees}

\subsection*{Problems}

\end{document}
